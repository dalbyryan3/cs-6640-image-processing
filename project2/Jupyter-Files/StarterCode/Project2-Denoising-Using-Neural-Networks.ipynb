{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Denoising\n",
    "\n",
    "Given a noisy image, we want to find a denoised version of the image. In this part of the project, You will do the following:-\n",
    "\n",
    "-  From true images, create a noisy training dataset\n",
    "-  Visualize Different types of noisy images and comment on the differences\n",
    "-  Using different models find denoised images.\n",
    "-  Visualize noisy and their denoised counter parts for atleast 2-5 input samples for analysis.\n",
    "-  Analyze which models give good results and why ?\n",
    "\n",
    "*** Extra Python libraries(Pytorch, pandas, csv) are needed for this assignment. So please ensure the cell directly below excutes properly before proceeding with the assignment. If it doesn't please make sure to install those libraries before hand. ***\n",
    "\n",
    "*** If you don't have access to GPU on your computer then install a CPU version of the pytorch library.  Required comments are provided to run the code on a CPU machine. Training on CPU is consideraly slower, so we would advise  you to use CADE machine for this assignment. CADE machine as 4GB GPU's which are enough for this project. ***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import skimage as ski\n",
    "from skimage import io, transform\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "sys.path.insert(1, '../../utils')\n",
    "from utils import NoiseDatsetLoader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Adding Noise in the Images And Creating a Noisy Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def noiseAddtion(image,noiseIndicator):\n",
    "    # This function gets an image of size m x n. Range of values is (0,1).\n",
    "    # noiseIndicator is type of noise that needs to be added to the image\n",
    "    # noiseIndicator == 0 indicates an addition of gaussian noise with mean 0 and var 0.08\n",
    "    # noiseIndicator == 1 indicates an addition of Salt and Pepper noise with intensity variation of 0.08\n",
    "    # noiseIndicator == 2 indicates an addition of poisson noise\n",
    "    # noiseIndicator == 3 indicates an addition of Speckle noise of mean 0 and var 0.05\n",
    "    \n",
    "    ## This function should return a noisy version of the input image\n",
    "    \n",
    "    ##  ***************** Your Code starts here ***************** ##\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ## ***************** Your Code ends here ***************** ##\n",
    "    return noisy"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def writeCSV(csv_file_name, Trainingdictlist):\n",
    "    csv_columns = ['RefImageName','NoiseType','NoisyImage']\n",
    "    try:\n",
    "        with open(csv_file_name, 'w') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "            writer.writeheader()\n",
    "            for data in Trainingdictlist:\n",
    "                writer.writerow(data)\n",
    "    except IOError:\n",
    "        print(\"I/O error\")\n",
    "\n",
    "def createNoisyDataset(root_dir,train,numberOfSamples,noisyIndicatorLow,noisyIndicatorHigh):\n",
    "    if(train==1):\n",
    "        name_csv = pd.read_csv('../../utils/file_name_train.csv')\n",
    "        csv_file_name = \"TrainingDataSet.csv\"\n",
    "        directoryName = \"TrainingDataset/\"\n",
    "        Trainingdictlist = [dict() for x in range(numberOfSamples)]\n",
    "        if not os.path.exists(directoryName):\n",
    "            os.makedirs(directoryName)\n",
    "    else:\n",
    "        name_csv = pd.read_csv('../../utils/file_name_test.csv')\n",
    "        csv_file_name = \"TestingDataSet.csv\"\n",
    "        directoryName = \"TestingDataset/\"\n",
    "        Trainingdictlist = [dict() for x in range(numberOfSamples)]\n",
    "        if not os.path.exists(directoryName):\n",
    "            os.makedirs(directoryName)\n",
    "\n",
    "    for i in range(numberOfSamples):\n",
    "        r2 = random.randint(0,len(name_csv)-1) # Choose an image randomly from the dataset\n",
    "        # Read the image from a path\n",
    "        img_name = os.path.join(root_dir,name_csv.iloc[r2, 0])\n",
    "        image    = io.imread(img_name)\n",
    "\n",
    "        # Normalize the image to range (0,1) \n",
    "        M,N      = image.shape\n",
    "        maximumPixelValue = np.max(image)\n",
    "        image = image/maximumPixelValue\n",
    "\n",
    "        # Choosing the noise randomly \n",
    "        r1 = random.randint(noisyIndicatorLow,noisyIndicatorHigh)\n",
    "        noisyImage = noiseAddtion(image,r1)\n",
    "        Trainingdictlist[i]={'RefImageName':img_name,'NoiseType': r1,'NoisyImage':str(i)+'.png'}\n",
    "        io.imsave(directoryName+str(i)+'.png',noisyImage)\n",
    "    writeCSV(csv_file_name, Trainingdictlist)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The function below calls the above function to create a noisy training and testing dataset. \n",
    "\n",
    "Don't worry about lossy conversion warnings."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "createNoisyDataset('../../utils/GrayScale',1,400,0,5) ## Creating 400 samples of Training Data\n",
    "createNoisyDataset('../../utils/GrayScale',0,200,0,5) ## Creating 200 samples of Training Data"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize Samples of Each of the noise Type here Using any plotting library\n",
    "\n",
    "Hint:- You can use Subplots to display different noisy images"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Your Code Starts here\n",
    "\n",
    "\n",
    "## Your Code ends here"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comment on the Differences of different sample of noisy images\n",
    "\n",
    "- Here"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network for Denoising\n",
    "\n",
    "We now have a noisy training set and a reference images."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dtype = torch.float32 ## instialize the data types used in training\n",
    "cpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Complete the Training Function Below"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def trainingLoop(dataloader,model,optimizer,nepochs):\n",
    "    ## This function Trains your model for 'nepochs'\n",
    "    ## Using optimizer and model your passed as arguments\n",
    "    ## On the data present in the DataLoader class passed\n",
    "    ##\n",
    "    ## This function the final loss value\n",
    "\n",
    "    model = model.to(device=cpu)\n",
    "    \n",
    "    ## Our Loss function for this exercise is fixed to MSELoss\n",
    "    loss_function = nn.MSELoss()\n",
    "    loss_array =[]\n",
    "    for e in range(nepochs):\n",
    "            print(\"Epoch\", e)\n",
    "            for t, temp in enumerate(dataloader):\n",
    "                ## Before Training Starts, put model in Training Mode\n",
    "                model.train()\n",
    "                NoisyImage = temp['NoisyImage'].to(device=cpu,dtype=dtype)\n",
    "                referenceImage = temp['image'].to(device=cpu,dtype=dtype)\n",
    "                ## Pass your input images through the model\n",
    "                ## Be sure to set the gradient as Zero in Optmizer before backward pass. Hint:- zero_grad()\n",
    "                ## Step the otimizer on after backward pass\n",
    "                ## Assign the value computed by the loss function to a varible named 'loss'\n",
    "\n",
    "                ## Due to dataset being Gray you may have to use unsqueeze function here\n",
    "\n",
    "                ## ************* Start of your Code *********************** ##\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                ## ************ End of your code ********************   ##\n",
    "                loss_array.append(loss.cpu().detach().numpy())\n",
    "            print(\"Training loss: \",loss)\n",
    "    return loss, loss_array"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "TrainingSet = NoiseDatsetLoader(csv_file='TrainingDataSet.csv', root_dir_noisy='TrainingDataset')\n",
    "TestingSet  = NoiseDatsetLoader(csv_file='TestingDataSet.csv' , root_dir_noisy='TestingDataset')\n",
    "\n",
    "## Batch Size is a hyper parameter, You may need to play with this paramter to get a more better network\n",
    "## This is a user controlled parameter you can change this value\n",
    "batch_size=16\n",
    "\n",
    "## DataLoader is a pytorch Class for iterating over a dataset\n",
    "dataloader_train  = DataLoader(TrainingSet,batch_size=batch_size,num_workers=4)\n",
    "dataloader_test   = DataLoader(TestingSet,batch_size=1)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model 1 \n",
    "\n",
    "- Declare a model with one conv2d filter with 1 input channel and output channel"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## ************* Start of your Code *********************** ##\n",
    "\n",
    "model = \n",
    "\n",
    "## ************ End of your code ********************   ##\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Set your hypter parameters\n",
    "learningRate =  # Set this value between 1e-2 to 1e-4\n",
    "weightDecay  =  # Set this value between 1e-2 to 1e-4\n",
    "\n",
    "epochs =  # Start with 50 or 100 and can experiment with value as high as 300-400.\n",
    "\n",
    "## Optimizer\n",
    "## Please Declare An Optimizer for your model. We suggest you use SGD\n",
    "## ************* Start of your Code *********************** ##\n",
    "\n",
    "optimizer =\n",
    "\n",
    "## ************ End of your code ********************   ##"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "valMSE, loss_array = trainingLoop(dataloader_train,model,optimizer,epochs)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot the graph of loss vs epoch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## ************* Start of your Code *********************** ##\n",
    "\n",
    "\n",
    "\n",
    "## ************ End of your code ********************   ##"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Complete the Accuracy Function Below "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def checkTestingAccuracy(dataloader,model):\n",
    "    ## This Function Checks Accuracy on Testing Dataset for the model passed\n",
    "    ## This function should return the loss the Testing Dataset\n",
    "\n",
    "    ## Before running on Testing Dataset the model should be in Evaluation Mode  \n",
    "    \n",
    "    model.eval()\n",
    "    totalLoss = []\n",
    "    loss_mse = nn.MSELoss()\n",
    "    for t, temp in enumerate(dataloader):\n",
    "        NoisyImage = temp['NoisyImage'].to(device=cpu,dtype=dtype)\n",
    "        referenceImage = temp['image'].to(device=cpu,dtype=dtype)\n",
    "\n",
    "        ## For Each Test Image Calculate the MSE loss with respect to Reference Image \n",
    "        ## Return the mean the total loss on the whole Testing Dataset\n",
    "        ## ************* Start of your Code *********************** ##\n",
    "        \n",
    "        \n",
    "        ## ************ End of your code ********************   ##"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Test Your Model. Complete the implementation of checkTestingAccuracy function above \n",
    "model.eval()\n",
    "testMSE = checkTestingAccuracy(dataloader_test,model)\n",
    "print(\"Mean Square Error for the testing Set for the trained model is \", testMSE)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot some of the Testing Dataset images by passing them through the trained model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## ************* Start of your Code *********************** ##\n",
    "\n",
    "\n",
    "## ************ End of your code ********************   ##"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comment on the Denoised image Obtained\n",
    "\n",
    "- Here"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 2\n",
    "\n",
    "- Declare a model with five conv2d filters, with input channel size of first filter as 1 and output channel size of last filter as 1. \n",
    "- All other intermediate channels you can change as you see fit( use a maximum of 8 or 16 channel inbetween layers, otherwise the model might take a huge amount of time to train). \n",
    "- Add batchnorm2d layers between each convolution layer for faster convergence."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## ************* Start of your Code *********************** ##\n",
    "\n",
    "model2 =\n",
    "\n",
    "## ************ End of your code ********************   ##"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Set your hypter parameters\n",
    "learningRate =   # Set this value between 1e-2 to 1e-4\n",
    "weightDecay  =   # Set this value between 1e-2 to 1e-4\n",
    "\n",
    "epochs =  # Start with 50 or 100 Can experiment with maximum 200 or 400 iterations\n",
    "\n",
    "## Optimizer\n",
    "## Please Declare An Optimizer for your model. We suggest you use SGD\n",
    "## ************* Start of your Code *********************** ##\n",
    "\n",
    "optimizer =\n",
    "\n",
    "## ************ End of your code ********************   ##"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "valMSE, loss_array = trainingLoop(dataloader_train,model2,optimizer,epochs)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Test Your Model\n",
    "model2.eval()\n",
    "testMSE = checkTestingAccuracy(dataloader_test,model2)\n",
    "print(\"Mean Square Error for the testing Set for the trained model is \", testMSE)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot some of the Testing Dataset images by passing them through the trained model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## ************* Start of your Code *********************** ##\n",
    "\n",
    "\n",
    "\n",
    "## ************ End of your code ********************   ##"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 3\n",
    "\n",
    "- Add Non Linear activations((preferably ReLU) Between conv2d layers from Model 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## ************* Start of your Code *********************** ##\n",
    "\n",
    "model3 =\n",
    "\n",
    "## ************* End of your Code *********************** ##"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Set your hypter parameters\n",
    "learningRate =   # Set this value between 1e-2 to 1e-4\n",
    "weightDecay  =   # Set this value between 1e-2 to 1e-4\n",
    "\n",
    "epochs =  # Start with 50 or 100 Can experiment with maximum 200 or 400 iterations\n",
    "\n",
    "## Optimizer\n",
    "## Please Declare An Optimizer for your model. We suggest you use SGD\n",
    "## ************* Start of your Code *********************** ##\n",
    "optimizer = \n",
    "## ************ End of your code ********************   ##"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "valMSE, loss_array = trainingLoop(dataloader_train,model3,optimizer,epochs)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Test Your Model\n",
    "model3.eval()\n",
    "testMSE = checkTestingAccuracy(dataloader_test,model3)\n",
    "print(\"Mean Square Error for the testing Set for the trained model is \", testMSE)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot some of the Testing Dataset images by passing them through the trained model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## ************* Start of your Code *********************** ##\n",
    "\n",
    "\n",
    "## ************ End of your code ********************   ##"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis\n",
    "\n",
    "Now that you have all the denoised outputs from different models, Answer the following questions.\n",
    "\n",
    "a. Which Model performed best and why ??"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "interpreter": {
   "hash": "7e57e8f265abc774e686117446c2fbb2d419dfdf6c72eebb754a4ed2f577b200"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}